This repository contains the official implementation for the paper: "Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety." This research explores using knowledge distillation (KD) to improve the safety of Large Language Models (LLMs) against multilingual jailbreak attacks and uncovers a counterintuitive degradation in model safety.

Our core experiment involved fine-tuning three student models—Llama-3-8B, Gemma-2-2B, and Qwen3-8B—on a dataset of "safe" refusal responses generated by a teacher model (OpenAI o1-mini). The goal was to teach the student models how to refuse harmful, multilingual prompts.

Follow instructions in MAIN.ipynb to run code.

Anonymous Submission
