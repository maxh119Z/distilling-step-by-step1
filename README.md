# Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety

This repository contains the official implementation for the paper: "Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety." This research explores using knowledge distillation (KD) to improve the safety of Large Language Models (LLMs) against multilingual jailbreak attacks and uncovers a counterintuitive degradation in model safety.

Our core experiment involved fine-tuning three student modelsâ€”`Llama-3-8B`, `Gemma-2-2B`, and `Qwen3-8B`â€”on a dataset of "safe" refusal responses generated by a teacher model (`OpenAI o1-mini`). The goal was to teach the student models how to refuse harmful, multilingual prompts. Contrary to our initial hypothesis, the fine-tuning process systematically weakened the models' safety alignment.

### Key Findings

**Safety Degradation**: Fine-tuning on "safe" refusal data from the teacher model inadvertently increased the Jailbreak Success Rate (JSR) for all student models by up to 16.6 percentage points
**Reasoning-Safety Trade-off**: The attempt to instill safety led to a consistent decline in general reasoning capabilities across all models, as measured by the GSM8K benchmark
**Causal Analysis**: We attribute the observed degradation to a combination of factors including nuanced 'boundary' data, amplification of teacher vulnerabilities, and catastrophic forgetting.

### Methodology Overview

Our experimental pipeline consists of five key stages:

1.  **Data Curation**: We sourced ~28,000 multilingual jailbreak prompts from the XSafety dataset (https://github.com/Jarviswang94/Multilingual_safety_benchmark).
2.  **Teacher Response Generation**: The teacher model, `OpenAI o1-mini`, generated safe refusal responses for these prompts.
3.  **Distillation Dataset Creation**: We paired the prompts and the teacher's refusals to create the fine-tuning dataset.
4.  **LoRA PEFT**: We used Low-Rank Adaptation (LoRA) to efficiently fine-tune the student models on this datasets.
5.  **Evaluation**: We evaluated the models on the MultiJail benchmark, using `GPT-4o` as an automated judge to classify responses as safe, unsafe, or invalid.

---

### Getting Started

Follow the instructions in `MAIN.ipynb` to run the code. The notebook is designed to be run cell-by-cell.

Before running, you must add your secret keys for Hugging Face and OpenAI in the notebook.

### ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ MAIN.ipynb            # Main notebook to run the full pipeline
â”œâ”€â”€ run.py                # Controller script that loads data and starts training
â”œâ”€â”€ train_utils.py        # Handles the core model loading, LoRA setup, and Trainer execution
â”œâ”€â”€ data_utils.py         # Contains data loader classes, including our custom SafetyDatasetLoader
â”œâ”€â”€ metrics.py            # Utility functions for computing metrics (not used in final training)
â”œâ”€â”€ README.md             # This README file
â””â”€â”€ LICENSE               # The Apache 2.0 License for the codebase
```

### ğŸ™ Acknowledgments

The code framework was adapted from Google's [Distilling-step-by-step](https://github.com/google-research/distilling-step-by-step).
